{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LearnTransformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyeTq1j8NeTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWQGJzRkPyI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# multi-head scaled dot-product attention\n",
        "class MultiHeadedScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.n = n_head\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.temperature = np.sqrt(d_k)\n",
        "        # self.temperature = torch.tensor(np.sqrt(d_k), dtype=torch.float, requires_grad=True)\n",
        "        self.wq = nn.Linear(d_model, d_k * n_head)\n",
        "        self.wk = nn.Linear(d_model, d_k * n_head)\n",
        "        self.wv = nn.Linear(d_model, d_v * n_head)\n",
        "        self.output = nn.Linear(d_v * n_head, d_model)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, q, k, v, residual=None, mask=None):\n",
        "        if residual is None:\n",
        "            residual = q\n",
        "        # (batch_size, n, seq_length, d)\n",
        "        q = self.wq(q).view(q.size(0), q.size(1), self.n, self.d_k).transpose(1, 2)\n",
        "        k = self.wk(k).view(k.size(0), k.size(1), self.n, self.d_k).transpose(1, 2)\n",
        "        v = self.wv(v).view(v.size(0), v.size(1), self.n, self.d_v).transpose(1, 2)\n",
        "        attn = torch.matmul(q, k.transpose(2, 3))\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask.unsqueeze(1)==0, -np.inf)\n",
        "        attn = F.softmax(attn / self.temperature, dim=-1)\n",
        "        attn_dot_v = torch.matmul(attn, v).transpose(1, 2).reshape(q.size(0), -1, self.d_v * self.n)\n",
        "        output = self.output(attn_dot_v)\n",
        "        output = self.dropout(output)\n",
        "        output = self.norm(output + residual)\n",
        "        return output, attn\n",
        "\n",
        "# positionwise feed-forward\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_hidden, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.L1 = nn.Linear(d_model, d_hidden)\n",
        "        self.L2 = nn.Linear(d_hidden, d_model)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        output = self.L2(F.relu(self.L1(x)))\n",
        "        output = self.dropout(output)\n",
        "        output = self.norm(output + residual)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfDZVGBDRrOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoder layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, d_hidden, n_head, d_k, d_v, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.slf_attn = MultiHeadedScaledDotProductAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
        "        self.pos_ffnn = PositionwiseFeedForward(d_model, d_hidden, dropout=dropout)\n",
        "\n",
        "    def forward(self, enc_input, slf_attn_mask=None):\n",
        "        slf_attn_output, slf_attn = self.slf_attn(enc_input, enc_input, enc_input, mask=slf_attn_mask)\n",
        "        enc_output = self.pos_ffnn(slf_attn_output)\n",
        "        return enc_output, slf_attn\n",
        "\n",
        "# decoder layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, d_hidden, n_head, d_k, d_v, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.slf_attn = MultiHeadedScaledDotProductAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
        "        self.enc_dec_attn = MultiHeadedScaledDotProductAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
        "        self.pos_ffnn = PositionwiseFeedForward(d_model, d_hidden, dropout=dropout)\n",
        "\n",
        "    def forward(self, dec_input, enc_output, slf_attn_mask=None, enc_dec_attn_mask=None):\n",
        "        slf_attn_output, slf_attn = self.slf_attn(dec_input, dec_input, dec_input, mask=slf_attn_mask)\n",
        "        enc_dec_attn_output, enc_dec_attn = self.enc_dec_attn(slf_attn_output, enc_output, enc_output, mask=enc_dec_attn_mask)\n",
        "        dec_output = self.pos_ffnn(enc_dec_attn_output)\n",
        "        return dec_output, slf_attn, enc_dec_attn\n",
        "\n",
        "# positional encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, n_position=100):\n",
        "        super().__init__()\n",
        "        self.pos_table = self._sinusoid_encoding_table(n_position, d_model)\n",
        "\n",
        "    def _sinusoid_encoding_table(self, n_position, d_model):\n",
        "        sinusoid_table = np.array([pos / np.power(10000, np.arange(d_model) // 2 * 2 / d_model)\n",
        "            for pos in range(n_position)], dtype=np.float)\n",
        "        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) \n",
        "        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
        "        # (batch_size, n_position, d_model)\n",
        "        return torch.tensor(sinusoid_table, dtype=torch.float).unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pos_table[:, :x.size(1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqGesiG1sOru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_src_vocab, d_model, d_hidden, n_head, d_k, d_v, n_position=100, n_layer=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(n_src_vocab, d_model, padding_idx=0)\n",
        "        self.pos_enc = PositionalEncoding(d_model, n_position=n_position)\n",
        "        self.enc_layer_stack = nn.ModuleList([EncoderLayer(d_model, d_hidden, n_head, d_k, d_v, dropout=dropout)\n",
        "            for _ in range(n_layer)])\n",
        "\n",
        "    def forward(self, src_seq, src_mask=None, return_attns=False):\n",
        "        enc_slf_attn_list = []\n",
        "        \n",
        "        enc_output = self.pos_enc(self.embedding(src_seq))\n",
        "        for enc_layer in self.enc_layer_stack:\n",
        "            enc_output, enc_slf_attn = enc_layer(enc_output, slf_attn_mask=src_mask)\n",
        "            enc_slf_attn_list += [enc_slf_attn] if return_attns else []\n",
        "        \n",
        "        return enc_output, enc_slf_attn_list\n",
        "\n",
        "# decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_trg_vocab, d_model, d_hidden, n_head, d_k, d_v, n_position=100, n_layer=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(n_trg_vocab, d_model, padding_idx=0)\n",
        "        self.pos_enc = PositionalEncoding(d_model, n_position=n_position)\n",
        "        self.dec_layer_stack = nn.ModuleList([DecoderLayer(d_model, d_hidden, n_head, d_k, d_v, dropout=dropout)\n",
        "            for _ in range(n_layer)])\n",
        "        \n",
        "    def forward(self, trg_seq, trg_mask, enc_output, src_mask, return_attns=False):\n",
        "        dec_slf_attn_list, enc_dec_attn_list = [], []\n",
        "\n",
        "        dec_output = self.pos_enc(self.embedding(trg_seq))\n",
        "        for dec_layer in self.dec_layer_stack:\n",
        "            dec_output, dec_slf_attn, enc_dec_attn = dec_layer(dec_output, enc_output, slf_attn_mask=trg_mask, enc_dec_attn_mask=src_mask)\n",
        "            dec_slf_attn_list += [dec_slf_attn] if return_attns else []\n",
        "            enc_dec_attn_list += [enc_dec_attn] if return_attns else []\n",
        "        \n",
        "        return dec_output, dec_slf_attn_list, enc_dec_attn_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6BkhYD14bBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transformer\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, n_src_vocab, n_trg_vocab, d_model=256, d_hidden=1024, \n",
        "                 n_head=8, d_k=64, d_v=64, n_position=100, n_layer=2, dropout=0.1, \n",
        "                 emb_src_trg_weight_sharing=True, trg_emb_prj_weight_sharing=True):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(n_src_vocab, d_model, d_hidden, n_head, d_k, d_v, \n",
        "                               n_position=n_position, n_layer=n_layer, dropout=dropout)\n",
        "        self.decoder = Decoder(n_trg_vocab, d_model, d_hidden, n_head, d_k, d_v, \n",
        "                               n_position=n_position, n_layer=n_layer, dropout=dropout)\n",
        "        self.trg_prj = nn.Linear(d_model, n_trg_vocab, bias=False)\n",
        "\n",
        "        if emb_src_trg_weight_sharing:\n",
        "            self.encoder.embedding.weight = self.decoder.embedding.weight\n",
        "        if trg_emb_prj_weight_sharing:\n",
        "            self.trg_prj.weight = self.decoder.embedding.weight\n",
        "            self.x_logit_scale = d_model ** -0.5\n",
        "\n",
        "    def forward(self, src_seq, src_mask: torch.Tensor, trg_seq, trg_mask: torch.Tensor):\n",
        "        enc_output, *_ = self.encoder(src_seq, src_mask)\n",
        "        dec_output, *_ = self.decoder(trg_seq, trg_mask, enc_output, src_mask)\n",
        "        transformer_output = F.softmax(self.trg_prj(dec_output) * self.x_logit_scale, dim=-1)\n",
        "\n",
        "        return transformer_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp_n9DoCcHHL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "bd010e30-62e4-4e96-d96d-10ed79df7e1c"
      },
      "source": [
        "model = Transformer(30, 30)\n",
        "x1 = torch.tensor([[3, 8, 9, 2]])\n",
        "x2 = torch.tensor([[1, 5, 19]])\n",
        "model(x1, None, x2, None)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[7.6401e-06, 9.9968e-01, 3.2029e-06, 2.4810e-06, 5.6179e-06,\n",
              "          2.2567e-06, 2.7340e-05, 7.3559e-07, 4.7759e-07, 8.8210e-06,\n",
              "          1.8307e-05, 1.0132e-05, 1.3958e-06, 2.8679e-06, 6.7776e-06,\n",
              "          6.4898e-06, 5.4635e-06, 1.5337e-05, 2.5972e-05, 1.0940e-05,\n",
              "          9.3544e-07, 2.8428e-06, 8.2941e-06, 5.0713e-06, 9.1464e-06,\n",
              "          9.1178e-05, 1.1957e-05, 5.3190e-06, 1.3683e-05, 6.6922e-06],\n",
              "         [4.1794e-06, 2.4963e-06, 1.1406e-06, 3.8322e-06, 1.4624e-05,\n",
              "          9.9983e-01, 2.1844e-05, 1.0107e-05, 1.8736e-06, 9.1035e-07,\n",
              "          1.1114e-05, 4.1653e-06, 1.0109e-05, 8.3385e-06, 1.6902e-06,\n",
              "          1.0906e-06, 1.7582e-05, 3.0540e-06, 1.0815e-06, 1.2542e-05,\n",
              "          5.4959e-06, 9.4522e-07, 1.9852e-06, 1.2144e-06, 9.0445e-07,\n",
              "          4.8632e-06, 3.4214e-06, 3.8293e-06, 1.5750e-05, 1.2982e-06],\n",
              "         [5.9970e-06, 6.7244e-06, 1.2330e-05, 5.8685e-06, 1.7898e-06,\n",
              "          1.0640e-05, 4.1920e-05, 3.3656e-06, 1.0078e-06, 1.8784e-06,\n",
              "          1.0348e-05, 8.8372e-06, 4.3256e-06, 7.8277e-06, 1.5864e-06,\n",
              "          6.7567e-06, 6.3986e-06, 2.1454e-05, 1.3843e-06, 9.9976e-01,\n",
              "          4.2673e-06, 4.8919e-06, 5.9926e-06, 1.5881e-05, 1.4066e-06,\n",
              "          1.5085e-05, 4.3714e-06, 3.0641e-06, 2.1901e-05, 1.6261e-06]]],\n",
              "       grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9BvLBNANwmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "class MnistDataset(data.Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        self.df = pd.read_csv(csv_file, header=None)\n",
        "    \n",
        "    def __len__(self): \n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.df.loc[idx].to_numpy()\n",
        "        label = data[0]\n",
        "        img = torch.from_numpy((data[1:] / 255).astype(np.float32))\n",
        "        return img, label\n",
        "\n",
        "class mnistTransformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = EncoderLayer(784, 256, 4, 16, 16)\n",
        "        self.output_layer = nn.Linear(784, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_output = self.encoder(x)\n",
        "        return F.softmax(self.output_layer(enc_output), dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUjL1gB59RXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = MnistDataset('/content/sample_data/mnist_train_small.csv')\n",
        "test_data = MnistDataset('/content/sample_data/mnist_test.csv')\n",
        "train_loader = data.DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "test_loader = data.DataLoader(test_data, batch_size=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u9RVncID9OY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = mnistTransformer()\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaSRLDq1-x8R",
        "colab_type": "code",
        "outputId": "4a8788cf-ea6b-4ce6-9dd0-727aa46a8806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for epoch in range(2):\n",
        "    for (x, y_true) in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x)\n",
        "        loss = loss_func(y_pred, y_true)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1250/1250 [00:12<00:00, 103.47it/s]\n",
            "100%|██████████| 1250/1250 [00:12<00:00, 103.05it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1qzp3dC_L2i",
        "colab_type": "code",
        "outputId": "aadce789-4840-435e-b32b-465a5a9f3b45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "for i, (X_test, y_test) in enumerate(test_loader):\n",
        "    y_pred = model(X_test)\n",
        "    correct += torch.argmax(y_pred, dim=-1).eq(y_test).sum()\n",
        "print(correct.item() / len(test_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9657\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCCs5IDRCDin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = 1 / torch.pow(10000, torch.arange(10) // 2 * 2 / 2.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kZaPKFeL9AQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}